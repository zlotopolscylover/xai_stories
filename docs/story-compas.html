<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 10 Story COMPAS: recidivism reloaded | XAI Stories</title>
  <meta name="description" content="Case studies for eXplainable Artificial Intelligence" />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 10 Story COMPAS: recidivism reloaded | XAI Stories" />
  <meta property="og:type" content="book" />
  
  <meta property="og:image" content="images/cover.png" />
  <meta property="og:description" content="Case studies for eXplainable Artificial Intelligence" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 10 Story COMPAS: recidivism reloaded | XAI Stories" />
  
  <meta name="twitter:description" content="Case studies for eXplainable Artificial Intelligence" />
  <meta name="twitter:image" content="images/cover.png" />



<meta name="date" content="2020-10-14" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="story-heloc-credits.html"/>
<link rel="next" href="acknowledgements.html"/>
<script src="libs/header-attrs/header-attrs.js"></script>
<script src="libs/jquery/jquery.min.js"></script>
<link href="libs/gitbook/css/style.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-clipboard.css" rel="stylesheet" />












<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./"><h3>XAI Stories</h3> Case studies</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="" data-path="foreword.html"><a href="foreword.html"><i class="fa fa-check"></i>Foreword</a>
<ul>
<li class="chapter" data-level="0.1" data-path="foreword.html"><a href="foreword.html#why"><i class="fa fa-check"></i><b>0.1</b> Why?</a></li>
<li class="chapter" data-level="0.2" data-path="foreword.html"><a href="foreword.html#what"><i class="fa fa-check"></i><b>0.2</b> What?</a></li>
<li class="chapter" data-level="0.3" data-path="foreword.html"><a href="foreword.html#how"><i class="fa fa-check"></i><b>0.3</b> How?</a></li>
<li class="chapter" data-level="0.4" data-path="foreword.html"><a href="foreword.html#about-academic-partners"><i class="fa fa-check"></i><b>0.4</b> About academic partners</a></li>
<li class="chapter" data-level="0.5" data-path="foreword.html"><a href="foreword.html#about-business-partner"><i class="fa fa-check"></i><b>0.5</b> About business partner</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="story-house-sale-prices.html"><a href="story-house-sale-prices.html"><i class="fa fa-check"></i><b>1</b> Story House Sale Prices: eXplainable predictions for house sale</a>
<ul>
<li class="chapter" data-level="" data-path="story-house-sale-prices.html"><a href="story-house-sale-prices.html#take-away-messages"><i class="fa fa-check"></i>Take-away messages</a></li>
<li class="chapter" data-level="1.1" data-path="story-house-sale-prices.html"><a href="story-house-sale-prices.html#introduction"><i class="fa fa-check"></i><b>1.1</b> Introduction</a></li>
<li class="chapter" data-level="1.2" data-path="story-house-sale-prices.html"><a href="story-house-sale-prices.html#data"><i class="fa fa-check"></i><b>1.2</b> Data</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="story-house-sale-prices.html"><a href="story-house-sale-prices.html#data-preperation"><i class="fa fa-check"></i><b>1.2.1</b> Data preperation</a></li>
<li class="chapter" data-level="1.2.2" data-path="story-house-sale-prices.html"><a href="story-house-sale-prices.html#external-data"><i class="fa fa-check"></i><b>1.2.2</b> External data</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="story-house-sale-prices.html"><a href="story-house-sale-prices.html#model"><i class="fa fa-check"></i><b>1.3</b> Model</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="story-house-sale-prices.html"><a href="story-house-sale-prices.html#linear-models"><i class="fa fa-check"></i><b>1.3.1</b> Linear models</a></li>
<li class="chapter" data-level="1.3.2" data-path="story-house-sale-prices.html"><a href="story-house-sale-prices.html#linear-models-assessment"><i class="fa fa-check"></i><b>1.3.2</b> Linear models assessment</a></li>
<li class="chapter" data-level="1.3.3" data-path="story-house-sale-prices.html"><a href="story-house-sale-prices.html#machine-learning-models"><i class="fa fa-check"></i><b>1.3.3</b> Machine Learning models</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="story-house-sale-prices.html"><a href="story-house-sale-prices.html#explanations"><i class="fa fa-check"></i><b>1.4</b> Explanations</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="story-house-sale-prices.html"><a href="story-house-sale-prices.html#xai-for-geographical-location"><i class="fa fa-check"></i><b>1.4.1</b> XAI for geographical location</a></li>
<li class="chapter" data-level="1.4.2" data-path="story-house-sale-prices.html"><a href="story-house-sale-prices.html#feature-importance"><i class="fa fa-check"></i><b>1.4.2</b> Feature importance</a></li>
<li class="chapter" data-level="1.4.3" data-path="story-house-sale-prices.html"><a href="story-house-sale-prices.html#partial-dependence-plots-pdp"><i class="fa fa-check"></i><b>1.4.3</b> Partial Dependence Plots (PDP)</a></li>
<li class="chapter" data-level="1.4.4" data-path="story-house-sale-prices.html"><a href="story-house-sale-prices.html#new-possibilities-with-pdp"><i class="fa fa-check"></i><b>1.4.4</b> New possibilities with PDP</a></li>
<li class="chapter" data-level="1.4.5" data-path="story-house-sale-prices.html"><a href="story-house-sale-prices.html#instance-level-explanations."><i class="fa fa-check"></i><b>1.4.5</b> Instance level explanations.</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="story-house-sale-prices.html"><a href="story-house-sale-prices.html#use-case-of-the-model"><i class="fa fa-check"></i><b>1.5</b> Use case of the model</a></li>
<li class="chapter" data-level="1.6" data-path="story-house-sale-prices.html"><a href="story-house-sale-prices.html#summary-and-conclusions"><i class="fa fa-check"></i><b>1.6</b> Summary and conclusions</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="story-hotel-booking.html"><a href="story-hotel-booking.html"><i class="fa fa-check"></i><b>2</b> Story Hotel Booking: eXplainable predictions of booking cancellation and guests coming back</a>
<ul>
<li class="chapter" data-level="2.1" data-path="story-hotel-booking.html"><a href="story-hotel-booking.html#introduction-1"><i class="fa fa-check"></i><b>2.1</b> Introduction</a></li>
<li class="chapter" data-level="2.2" data-path="story-hotel-booking.html"><a href="story-hotel-booking.html#booking-cancellation"><i class="fa fa-check"></i><b>2.2</b> Booking Cancellation</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="story-hotel-booking.html"><a href="story-hotel-booking.html#booking-cancellation-model"><i class="fa fa-check"></i><b>2.2.1</b> Booking Cancellation: Model</a></li>
<li class="chapter" data-level="2.2.2" data-path="story-hotel-booking.html"><a href="story-hotel-booking.html#booking-cancellation-explanation-dataset-level"><i class="fa fa-check"></i><b>2.2.2</b> Booking Cancellation: Explanation, dataset level</a></li>
<li class="chapter" data-level="2.2.3" data-path="story-hotel-booking.html"><a href="story-hotel-booking.html#booking-cancellation-explanations-instance-level"><i class="fa fa-check"></i><b>2.2.3</b> Booking Cancellation: Explanations, instance level</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="story-hotel-booking.html"><a href="story-hotel-booking.html#repeated-guests"><i class="fa fa-check"></i><b>2.3</b> Repeated guests</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="story-hotel-booking.html"><a href="story-hotel-booking.html#repeated-guests-imbalanced-dataset"><i class="fa fa-check"></i><b>2.3.1</b> Repeated guests: Imbalanced dataset</a></li>
<li class="chapter" data-level="2.3.2" data-path="story-hotel-booking.html"><a href="story-hotel-booking.html#repeated-guests-model"><i class="fa fa-check"></i><b>2.3.2</b> Repeated guests: Model</a></li>
<li class="chapter" data-level="2.3.3" data-path="story-hotel-booking.html"><a href="story-hotel-booking.html#repeated-guests-explanations-instance-level"><i class="fa fa-check"></i><b>2.3.3</b> Repeated guests: Explanations, instance level</a></li>
<li class="chapter" data-level="2.3.4" data-path="story-hotel-booking.html"><a href="story-hotel-booking.html#repeated-guests-explanations-dataset-level"><i class="fa fa-check"></i><b>2.3.4</b> Repeated guests: Explanations, dataset level</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="story-hotel-booking.html"><a href="story-hotel-booking.html#summary-and-conclusions-1"><i class="fa fa-check"></i><b>2.4</b> Summary and conclusions</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="story-hotel-booking-cancellations-explainable-predictions-for-booking-cancellation.html"><a href="story-hotel-booking-cancellations-explainable-predictions-for-booking-cancellation.html"><i class="fa fa-check"></i><b>3</b> Story Hotel Booking Cancellations: eXplainable predictions for booking cancellation</a>
<ul>
<li class="chapter" data-level="3.1" data-path="story-hotel-booking-cancellations-explainable-predictions-for-booking-cancellation.html"><a href="story-hotel-booking-cancellations-explainable-predictions-for-booking-cancellation.html#introduction-2"><i class="fa fa-check"></i><b>3.1</b> Introduction</a></li>
<li class="chapter" data-level="3.2" data-path="story-hotel-booking-cancellations-explainable-predictions-for-booking-cancellation.html"><a href="story-hotel-booking-cancellations-explainable-predictions-for-booking-cancellation.html#problem-specification"><i class="fa fa-check"></i><b>3.2</b> Problem specification</a></li>
<li class="chapter" data-level="3.3" data-path="story-hotel-booking-cancellations-explainable-predictions-for-booking-cancellation.html"><a href="story-hotel-booking-cancellations-explainable-predictions-for-booking-cancellation.html#target-leak-detection"><i class="fa fa-check"></i><b>3.3</b> Target leak detection</a></li>
<li class="chapter" data-level="3.4" data-path="story-hotel-booking-cancellations-explainable-predictions-for-booking-cancellation.html"><a href="story-hotel-booking-cancellations-explainable-predictions-for-booking-cancellation.html#bias-correction"><i class="fa fa-check"></i><b>3.4</b> Bias correction</a></li>
<li class="chapter" data-level="3.5" data-path="story-hotel-booking-cancellations-explainable-predictions-for-booking-cancellation.html"><a href="story-hotel-booking-cancellations-explainable-predictions-for-booking-cancellation.html#offering-different-conditions"><i class="fa fa-check"></i><b>3.5</b> Offering different conditions</a></li>
<li class="chapter" data-level="3.6" data-path="story-hotel-booking-cancellations-explainable-predictions-for-booking-cancellation.html"><a href="story-hotel-booking-cancellations-explainable-predictions-for-booking-cancellation.html#conclusions"><i class="fa fa-check"></i><b>3.6</b> Conclusions</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="story-uplift-modelling.html"><a href="story-uplift-modelling.html"><i class="fa fa-check"></i><b>4</b> Story Uplift Modelling: eXplaining colon cancer survival rate after treatment</a>
<ul>
<li class="chapter" data-level="4.1" data-path="story-uplift-modelling.html"><a href="story-uplift-modelling.html#introduction-3"><i class="fa fa-check"></i><b>4.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="story-uplift-modelling.html"><a href="story-uplift-modelling.html#what-is-uplift-modelling"><i class="fa fa-check"></i><b>4.1.1</b> What is Uplift Modelling?</a></li>
<li class="chapter" data-level="4.1.2" data-path="story-uplift-modelling.html"><a href="story-uplift-modelling.html#dataset-description"><i class="fa fa-check"></i><b>4.1.2</b> Dataset Description</a></li>
<li class="chapter" data-level="4.1.3" data-path="story-uplift-modelling.html"><a href="story-uplift-modelling.html#ideas"><i class="fa fa-check"></i><b>4.1.3</b> Ideas</a></li>
<li class="chapter" data-level="4.1.4" data-path="story-uplift-modelling.html"><a href="story-uplift-modelling.html#why-is-it-worth-the-hassle"><i class="fa fa-check"></i><b>4.1.4</b> Why is it worth the hassle?</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="story-uplift-modelling.html"><a href="story-uplift-modelling.html#data-preprocessing"><i class="fa fa-check"></i><b>4.2</b> Data Preprocessing</a></li>
<li class="chapter" data-level="4.3" data-path="story-uplift-modelling.html"><a href="story-uplift-modelling.html#model-1"><i class="fa fa-check"></i><b>4.3</b> Model</a></li>
<li class="chapter" data-level="4.4" data-path="story-uplift-modelling.html"><a href="story-uplift-modelling.html#explanations-1"><i class="fa fa-check"></i><b>4.4</b> Explanations</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="story-uplift-modelling.html"><a href="story-uplift-modelling.html#feature-importance-1"><i class="fa fa-check"></i><b>4.4.1</b> Feature Importance</a></li>
<li class="chapter" data-level="4.4.2" data-path="story-uplift-modelling.html"><a href="story-uplift-modelling.html#ceteris-paribus-and-partial-dependence-profiles"><i class="fa fa-check"></i><b>4.4.2</b> Ceteris Paribus and Partial Dependence Profiles</a></li>
<li class="chapter" data-level="4.4.3" data-path="story-uplift-modelling.html"><a href="story-uplift-modelling.html#instance-level-explainations"><i class="fa fa-check"></i><b>4.4.3</b> Instance Level Explainations</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="story-uplift-modelling.html"><a href="story-uplift-modelling.html#summary-and-conclusions-2"><i class="fa fa-check"></i><b>4.5</b> Summary and Conclusions</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="story-uplift-marketing1.html"><a href="story-uplift-marketing1.html"><i class="fa fa-check"></i><b>5</b> Story Uplift Modeling: eXplainable predictions for optimized marketing campaigns</a>
<ul>
<li class="chapter" data-level="5.1" data-path="story-uplift-marketing1.html"><a href="story-uplift-marketing1.html#introduction-4"><i class="fa fa-check"></i><b>5.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="story-uplift-marketing1.html"><a href="story-uplift-marketing1.html#approaches-towards-uplift-modeling"><i class="fa fa-check"></i><b>5.1.1</b> Approaches towards uplift modeling</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="story-uplift-marketing1.html"><a href="story-uplift-marketing1.html#dataset"><i class="fa fa-check"></i><b>5.2</b> Dataset</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="story-uplift-marketing1.html"><a href="story-uplift-marketing1.html#explanatory-data-analysis"><i class="fa fa-check"></i><b>5.2.1</b> Explanatory Data Analysis</a></li>
<li class="chapter" data-level="5.2.2" data-path="story-uplift-marketing1.html"><a href="story-uplift-marketing1.html#feature-engineering"><i class="fa fa-check"></i><b>5.2.2</b> Feature engineering</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="story-uplift-marketing1.html"><a href="story-uplift-marketing1.html#model-exploration-and-metrics"><i class="fa fa-check"></i><b>5.3</b> Model exploration and metrics</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="story-uplift-marketing1.html"><a href="story-uplift-marketing1.html#model-2"><i class="fa fa-check"></i><b>5.3.1</b> Model</a></li>
<li class="chapter" data-level="5.3.2" data-path="story-uplift-marketing1.html"><a href="story-uplift-marketing1.html#comparing-metrics"><i class="fa fa-check"></i><b>5.3.2</b> Comparing metrics</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="story-uplift-marketing1.html"><a href="story-uplift-marketing1.html#explanations-2"><i class="fa fa-check"></i><b>5.4</b> Explanations</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="story-uplift-marketing1.html"><a href="story-uplift-marketing1.html#instance-level"><i class="fa fa-check"></i><b>5.4.1</b> Instance-level</a></li>
<li class="chapter" data-level="5.4.2" data-path="story-uplift-marketing1.html"><a href="story-uplift-marketing1.html#difference---approach"><i class="fa fa-check"></i><b>5.4.2</b> Difference - approach</a></li>
<li class="chapter" data-level="5.4.3" data-path="story-uplift-marketing1.html"><a href="story-uplift-marketing1.html#dataset--subset--level"><i class="fa fa-check"></i><b>5.4.3</b> Dataset- (subset-) level</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="story-uplift-marketing1.html"><a href="story-uplift-marketing1.html#conclusions-1"><i class="fa fa-check"></i><b>5.5</b> Conclusions</a>
<ul>
<li class="chapter" data-level="5.5.1" data-path="story-uplift-marketing1.html"><a href="story-uplift-marketing1.html#sweet-spot"><i class="fa fa-check"></i><b>5.5.1</b> Sweet-spot</a></li>
<li class="chapter" data-level="5.5.2" data-path="story-uplift-marketing1.html"><a href="story-uplift-marketing1.html#bigger-influence"><i class="fa fa-check"></i><b>5.5.2</b> Bigger influence</a></li>
<li class="chapter" data-level="5.5.3" data-path="story-uplift-marketing1.html"><a href="story-uplift-marketing1.html#influence-from-other-factors"><i class="fa fa-check"></i><b>5.5.3</b> Influence from other factors</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="story-uplift-marketing1.html"><a href="story-uplift-marketing1.html#summary"><i class="fa fa-check"></i><b>5.6</b> Summary</a>
<ul>
<li class="chapter" data-level="5.6.1" data-path="story-uplift-marketing1.html"><a href="story-uplift-marketing1.html#individual-perspective"><i class="fa fa-check"></i><b>5.6.1</b> Individual perspective</a></li>
<li class="chapter" data-level="5.6.2" data-path="story-uplift-marketing1.html"><a href="story-uplift-marketing1.html#data-scientists-perspective"><i class="fa fa-check"></i><b>5.6.2</b> Data scientist’s perspective</a></li>
<li class="chapter" data-level="5.6.3" data-path="story-uplift-marketing1.html"><a href="story-uplift-marketing1.html#executive-perspective"><i class="fa fa-check"></i><b>5.6.3</b> Executive perspective</a></li>
</ul></li>
<li class="chapter" data-level="5.7" data-path="story-uplift-marketing1.html"><a href="story-uplift-marketing1.html#future-works"><i class="fa fa-check"></i><b>5.7</b> Future works</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="story-meps-explainable-predictions-for-healthcare-expenditures.html"><a href="story-meps-explainable-predictions-for-healthcare-expenditures.html"><i class="fa fa-check"></i><b>6</b> Story MEPS: Explainable predictions for healthcare expenditures</a>
<ul>
<li class="chapter" data-level="6.1" data-path="story-meps-explainable-predictions-for-healthcare-expenditures.html"><a href="story-meps-explainable-predictions-for-healthcare-expenditures.html#introduction-5"><i class="fa fa-check"></i><b>6.1</b> Introduction</a></li>
<li class="chapter" data-level="6.2" data-path="story-meps-explainable-predictions-for-healthcare-expenditures.html"><a href="story-meps-explainable-predictions-for-healthcare-expenditures.html#model-3"><i class="fa fa-check"></i><b>6.2</b> Model</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="story-meps-explainable-predictions-for-healthcare-expenditures.html"><a href="story-meps-explainable-predictions-for-healthcare-expenditures.html#data-1"><i class="fa fa-check"></i><b>6.2.1</b> Data</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="story-meps-explainable-predictions-for-healthcare-expenditures.html"><a href="story-meps-explainable-predictions-for-healthcare-expenditures.html#model-4"><i class="fa fa-check"></i><b>6.3</b> Model</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="story-meps-explainable-predictions-for-healthcare-expenditures.html"><a href="story-meps-explainable-predictions-for-healthcare-expenditures.html#data-2"><i class="fa fa-check"></i><b>6.3.1</b> Data</a></li>
<li class="chapter" data-level="6.3.2" data-path="story-meps-explainable-predictions-for-healthcare-expenditures.html"><a href="story-meps-explainable-predictions-for-healthcare-expenditures.html#model-5"><i class="fa fa-check"></i><b>6.3.2</b> Model</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="story-meps-explainable-predictions-for-healthcare-expenditures.html"><a href="story-meps-explainable-predictions-for-healthcare-expenditures.html#model-level-explainations"><i class="fa fa-check"></i><b>6.4</b> Model Level Explainations</a>
<ul>
<li class="chapter" data-level="6.4.1" data-path="story-meps-explainable-predictions-for-healthcare-expenditures.html"><a href="story-meps-explainable-predictions-for-healthcare-expenditures.html#permutation-variable-importances"><i class="fa fa-check"></i><b>6.4.1</b> Permutation Variable Importances</a></li>
<li class="chapter" data-level="6.4.2" data-path="story-meps-explainable-predictions-for-healthcare-expenditures.html"><a href="story-meps-explainable-predictions-for-healthcare-expenditures.html#partial-dependence-profiles"><i class="fa fa-check"></i><b>6.4.2</b> Partial Dependence Profiles</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="story-meps-explainable-predictions-for-healthcare-expenditures.html"><a href="story-meps-explainable-predictions-for-healthcare-expenditures.html#instance-level-explanations"><i class="fa fa-check"></i><b>6.5</b> Instance Level Explanations</a>
<ul>
<li class="chapter" data-level="6.5.1" data-path="story-meps-explainable-predictions-for-healthcare-expenditures.html"><a href="story-meps-explainable-predictions-for-healthcare-expenditures.html#business-approach"><i class="fa fa-check"></i><b>6.5.1</b> Business approach</a></li>
<li class="chapter" data-level="6.5.2" data-path="story-meps-explainable-predictions-for-healthcare-expenditures.html"><a href="story-meps-explainable-predictions-for-healthcare-expenditures.html#instance-level-explanations---instance-specific-approach"><i class="fa fa-check"></i><b>6.5.2</b> Instance Level Explanations - instance specific approach</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="story-meps-explainable-predictions-for-healthcare-expenditures.html"><a href="story-meps-explainable-predictions-for-healthcare-expenditures.html#summary-and-conclusions-3"><i class="fa fa-check"></i><b>6.6</b> Summary and conclusions</a></li>
<li class="chapter" data-level="6.7" data-path="story-meps-explainable-predictions-for-healthcare-expenditures.html"><a href="story-meps-explainable-predictions-for-healthcare-expenditures.html#references"><i class="fa fa-check"></i><b>6.7</b> References</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="story-meps-healthcare-expenditures-of-individuals.html"><a href="story-meps-healthcare-expenditures-of-individuals.html"><i class="fa fa-check"></i><b>7</b> Story MEPS: Healthcare expenditures of individuals</a>
<ul>
<li class="chapter" data-level="7.1" data-path="story-meps-healthcare-expenditures-of-individuals.html"><a href="story-meps-healthcare-expenditures-of-individuals.html#introduction-6"><i class="fa fa-check"></i><b>7.1</b> Introduction</a></li>
<li class="chapter" data-level="7.2" data-path="story-meps-healthcare-expenditures-of-individuals.html"><a href="story-meps-healthcare-expenditures-of-individuals.html#data-set"><i class="fa fa-check"></i><b>7.2</b> Data set</a></li>
<li class="chapter" data-level="7.3" data-path="story-meps-healthcare-expenditures-of-individuals.html"><a href="story-meps-healthcare-expenditures-of-individuals.html#models"><i class="fa fa-check"></i><b>7.3</b> Models</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="story-meps-healthcare-expenditures-of-individuals.html"><a href="story-meps-healthcare-expenditures-of-individuals.html#model-1-ridge-regression"><i class="fa fa-check"></i><b>7.3.1</b> Model 1: Ridge regression</a></li>
<li class="chapter" data-level="7.3.2" data-path="story-meps-healthcare-expenditures-of-individuals.html"><a href="story-meps-healthcare-expenditures-of-individuals.html#model-2-artificial-neural-network"><i class="fa fa-check"></i><b>7.3.2</b> Model 2: Artificial Neural Network</a></li>
<li class="chapter" data-level="7.3.3" data-path="story-meps-healthcare-expenditures-of-individuals.html"><a href="story-meps-healthcare-expenditures-of-individuals.html#model-3-gradient-boosting"><i class="fa fa-check"></i><b>7.3.3</b> Model 3: Gradient Boosting</a></li>
<li class="chapter" data-level="7.3.4" data-path="story-meps-healthcare-expenditures-of-individuals.html"><a href="story-meps-healthcare-expenditures-of-individuals.html#results"><i class="fa fa-check"></i><b>7.3.4</b> Results</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="story-meps-healthcare-expenditures-of-individuals.html"><a href="story-meps-healthcare-expenditures-of-individuals.html#analysis-with-xai-methods"><i class="fa fa-check"></i><b>7.4</b> Analysis with XAI methods</a>
<ul>
<li class="chapter" data-level="7.4.1" data-path="story-meps-healthcare-expenditures-of-individuals.html"><a href="story-meps-healthcare-expenditures-of-individuals.html#analysis-of-patients-with-zero-expenditures"><i class="fa fa-check"></i><b>7.4.1</b> Analysis of patients with zero expenditures</a></li>
<li class="chapter" data-level="7.4.2" data-path="story-meps-healthcare-expenditures-of-individuals.html"><a href="story-meps-healthcare-expenditures-of-individuals.html#interpretation-of-self-reported-health-status-and-survey-questions-design"><i class="fa fa-check"></i><b>7.4.2</b> Interpretation of self-reported health status and survey questions design</a></li>
<li class="chapter" data-level="7.4.3" data-path="story-meps-healthcare-expenditures-of-individuals.html"><a href="story-meps-healthcare-expenditures-of-individuals.html#the-relationship-between-age-and-health-expenditure"><i class="fa fa-check"></i><b>7.4.3</b> The relationship between age and health expenditure</a></li>
<li class="chapter" data-level="7.4.4" data-path="story-meps-healthcare-expenditures-of-individuals.html"><a href="story-meps-healthcare-expenditures-of-individuals.html#using-shap-for-encoding-and-grouping-of-categorical-variables"><i class="fa fa-check"></i><b>7.4.4</b> Using SHAP for encoding and grouping of categorical variables</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="story-meps-healthcare-expenditures-of-individuals.html"><a href="story-meps-healthcare-expenditures-of-individuals.html#conclusion"><i class="fa fa-check"></i><b>7.5</b> Conclusion</a></li>
<li class="chapter" data-level="7.6" data-path="story-meps-healthcare-expenditures-of-individuals.html"><a href="story-meps-healthcare-expenditures-of-individuals.html#appendix"><i class="fa fa-check"></i><b>7.6</b> Appendix</a>
<ul>
<li class="chapter" data-level="7.6.1" data-path="story-meps-healthcare-expenditures-of-individuals.html"><a href="story-meps-healthcare-expenditures-of-individuals.html#additionall-plots"><i class="fa fa-check"></i><b>7.6.1</b> Additionall plots</a></li>
<li class="chapter" data-level="7.6.2" data-path="story-meps-healthcare-expenditures-of-individuals.html"><a href="story-meps-healthcare-expenditures-of-individuals.html#variables-descriptions"><i class="fa fa-check"></i><b>7.6.2</b> Variables descriptions</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="story-lungs.html"><a href="story-lungs.html"><i class="fa fa-check"></i><b>8</b> Story Lungs: eXplainable predictions for post operational risks</a>
<ul>
<li class="chapter" data-level="8.1" data-path="story-lungs.html"><a href="story-lungs.html#introduction-7"><i class="fa fa-check"></i><b>8.1</b> Introduction</a></li>
<li class="chapter" data-level="8.2" data-path="story-lungs.html"><a href="story-lungs.html#data-set-1"><i class="fa fa-check"></i><b>8.2</b> Data set</a></li>
<li class="chapter" data-level="8.3" data-path="story-lungs.html"><a href="story-lungs.html#models-1"><i class="fa fa-check"></i><b>8.3</b> Models</a></li>
<li class="chapter" data-level="8.4" data-path="story-lungs.html"><a href="story-lungs.html#explanations-3"><i class="fa fa-check"></i><b>8.4</b> Explanations</a></li>
<li class="chapter" data-level="8.5" data-path="story-lungs.html"><a href="story-lungs.html#summary-and-conclusions-4"><i class="fa fa-check"></i><b>8.5</b> Summary and conclusions</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="story-heloc-credits.html"><a href="story-heloc-credits.html"><i class="fa fa-check"></i><b>9</b> Story HELOC Credits</a>
<ul>
<li class="chapter" data-level="9.1" data-path="story-heloc-credits.html"><a href="story-heloc-credits.html#introduction-8"><i class="fa fa-check"></i><b>9.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="9.1.1" data-path="story-heloc-credits.html"><a href="story-heloc-credits.html#dataset-1"><i class="fa fa-check"></i><b>9.1.1</b> Dataset</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="story-heloc-credits.html"><a href="story-heloc-credits.html#model-6"><i class="fa fa-check"></i><b>9.2</b> Model</a></li>
<li class="chapter" data-level="9.3" data-path="story-heloc-credits.html"><a href="story-heloc-credits.html#explanations-4"><i class="fa fa-check"></i><b>9.3</b> Explanations</a>
<ul>
<li class="chapter" data-level="9.3.1" data-path="story-heloc-credits.html"><a href="story-heloc-credits.html#model-explanations"><i class="fa fa-check"></i><b>9.3.1</b> Model explanations</a></li>
<li class="chapter" data-level="9.3.2" data-path="story-heloc-credits.html"><a href="story-heloc-credits.html#explanation-for-clients"><i class="fa fa-check"></i><b>9.3.2</b> Explanation for clients</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="story-heloc-credits.html"><a href="story-heloc-credits.html#summary-and-conclusion"><i class="fa fa-check"></i><b>9.4</b> Summary and conclusion</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="story-compas.html"><a href="story-compas.html"><i class="fa fa-check"></i><b>10</b> Story COMPAS: recidivism reloaded</a>
<ul>
<li class="chapter" data-level="" data-path="story-compas.html"><a href="story-compas.html#take-away-messages-1"><i class="fa fa-check"></i>Take-away messages</a></li>
<li class="chapter" data-level="10.1" data-path="story-compas.html"><a href="story-compas.html#introduction-9"><i class="fa fa-check"></i><b>10.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="10.1.1" data-path="story-compas.html"><a href="story-compas.html#previous-work-on-compas-algorithm"><i class="fa fa-check"></i><b>10.1.1</b> Previous work on COMPAS algorithm</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="story-compas.html"><a href="story-compas.html#data-3"><i class="fa fa-check"></i><b>10.2</b> Data</a></li>
<li class="chapter" data-level="10.3" data-path="story-compas.html"><a href="story-compas.html#models-2"><i class="fa fa-check"></i><b>10.3</b> Models</a>
<ul>
<li class="chapter" data-level="10.3.1" data-path="story-compas.html"><a href="story-compas.html#results---compas-regression"><i class="fa fa-check"></i><b>10.3.1</b> Results - COMPAS Regression</a></li>
<li class="chapter" data-level="10.3.2" data-path="story-compas.html"><a href="story-compas.html#results---recidivism-classification"><i class="fa fa-check"></i><b>10.3.2</b> Results - Recidivism Classification</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="story-compas.html"><a href="story-compas.html#explanations-5"><i class="fa fa-check"></i><b>10.4</b> Explanations</a>
<ul>
<li class="chapter" data-level="10.4.1" data-path="story-compas.html"><a href="story-compas.html#model-specific"><i class="fa fa-check"></i><b>10.4.1</b> Model specific</a></li>
<li class="chapter" data-level="10.4.2" data-path="story-compas.html"><a href="story-compas.html#instance-specific"><i class="fa fa-check"></i><b>10.4.2</b> Instance specific</a></li>
<li class="chapter" data-level="10.4.3" data-path="story-compas.html"><a href="story-compas.html#models-without-protected-attributes"><i class="fa fa-check"></i><b>10.4.3</b> Models without protected attributes</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="story-compas.html"><a href="story-compas.html#fairness"><i class="fa fa-check"></i><b>10.5</b> Fairness</a>
<ul>
<li class="chapter" data-level="10.5.1" data-path="story-compas.html"><a href="story-compas.html#overview"><i class="fa fa-check"></i><b>10.5.1</b> Overview</a></li>
<li class="chapter" data-level="10.5.2" data-path="story-compas.html"><a href="story-compas.html#models-3"><i class="fa fa-check"></i><b>10.5.2</b> Models</a></li>
</ul></li>
<li class="chapter" data-level="10.6" data-path="story-compas.html"><a href="story-compas.html#summary-and-conclusions-5"><i class="fa fa-check"></i><b>10.6</b> Summary and conclusions</a>
<ul>
<li class="chapter" data-level="10.6.1" data-path="story-compas.html"><a href="story-compas.html#next-steps"><i class="fa fa-check"></i><b>10.6.1</b> Next steps</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="acknowledgements.html"><a href="acknowledgements.html"><i class="fa fa-check"></i>Acknowledgements</a></li>
<li class="chapter" data-level="" data-path="references-1.html"><a href="references-1.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="_blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">XAI Stories</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="story-compas" class="section level1" number="10">
<h1><span class="header-section-number">Chapter 10</span> Story COMPAS: recidivism reloaded</h1>
<p><em>Authors: Łukasz Grad (University of Warsaw), Katarzyna Koprowska (Warsaw University of Technology)</em></p>
<p><em>Mentors: Michał Miktus (McKinsey &amp; Company)</em></p>
<div id="take-away-messages-1" class="section level3 unnumbered">
<h3>Take-away messages</h3>
<ul>
<li>Based on limited publicly available data COMPAS scores can only be reconstructed approximately.</li>
<li>Direct recidivism modelling provides better results than both raw COMPAS scores and models trained on COMPAS.</li>
<li>Race is an influential factor when predicting recidivism.</li>
<li>Removing bias from a model is a complex task.</li>
<li>Our efforts to mitigate the racial bias resulted in a relatively fair model, but at the cost of decreased model performance, suggesting that some trade-offs are inevitable.</li>
</ul>
</div>
<div id="introduction-9" class="section level2" number="10.1">
<h2><span class="header-section-number">10.1</span> Introduction</h2>
<p>The study covers the problem of assessing the likelihood of a defendant becoming a recidivist. One of the best known solutions currently used on a daily basis is COMPAS, an acronym for Correctional Offender Management Profiling for Alternative Sanctions, created by a for-profit company Northpointe.</p>
<p>COMPAS is a widely popular commercial algorithm used by judges and parole officers for scoring a criminal defendant’s likelihood of recidivism. It was designed to help judges identify potentially more dangerous individuals and award them with longer sentences. It is easy to notice that COMPAS results have fundamental consequences for the lives of many United States residents.</p>
<p>However, the algorithm is, due to its proprietary nature, still a black-box for the wide audience – meaning that we cannot easily identify which factors did it consider when classifying an individual as a person with a high or low likelihood of reoffending.</p>
<p>Consequently, many questions have arised about the fairness of the algorithm.</p>
<p>In this study, we try to answer the following questions:</p>
<ul>
<li>Can we reconstruct COMPAS scores given our limited data about defendants?</li>
<li>What are important factors contributing to the COMPAS score?</li>
<li>Can we improve on the COMPAS by using a more complex model?</li>
<li>Is our model fair or does it show racial bias?</li>
<li>How can we remove racial bias and what is the impact of such operation on model accuracy?</li>
</ul>
<p>The rest of the article is organized as follows. Next subsection briefly discusses previous attempts at both analyzing COMPAS algorithm in the context of racial bias, as well as the COMPAS scores reconstruction. In section 2 we describe the data we used for modelling along with variable description. In section 3 we present the estimated models and detailed description of model fitting approach. Moreover, it contains results for direct recidivism modelling as well as attempts at COMPAS score reconstruction. In section 4 we show global and instance level explanations in order to assess the degree of racial bias present in our models. Section 5 contains quantitative fairness analysis augmented with an attempt to remove bias from our best-performing model. Lastly, we briefly mention potential next steps and finish with a summary of our results.</p>
<div id="previous-work-on-compas-algorithm" class="section level3" number="10.1.1">
<h3><span class="header-section-number">10.1.1</span> Previous work on COMPAS algorithm</h3>
<p>One of the first and most known investigators aiming to validate COMPAS results was ProPublica group <span class="citation">Larson et al. (<a href="#ref-propublica" role="doc-biblioref">2016</a>)</span>. They have provoked a vigorous discussion about the fairness of black-box models with their 2016 study, which attempted to reconstruct COMPAS methodology. They have collected criminal records from Broward County FL for several thousand people, as well as reports about their offenses in a two-year follow-up period. The overall accuracy of ProPublica’s reconstructed model was only 61%, but their main discovery was a racial bias in favour of defendants of Caucasian origins over those of African-American origins. According to the study, black defendants were particularly likely to be falsely flagged as future criminals almost twice as often as white ones, who were also more often mislabeled as low risk. The researchers believed that the aforementioned disparity cannot be explained by either defendant’s prior crimes, their type, gender or age.</p>
<p>The ProPublica study was, however, heavily criticized for its flawed methodology. One of the critics was Northpointe, the creator of the COMPAS algorithm, who defended the accuracy of its test, because the results from ProPublica do not precisely reflect their model.</p>
<p>After ProPublica’s publication, confusion and doubts whether COMPAS should still be relied on, began to appear among researchers, leading several of them to propose their own validations of the algorithm, based on data provided by ProPublica. One of the most reliable work seems to be the study “The age of secrecy and unfairness in recidivism prediction” by Rudin et al. <span class="citation">(<a href="#ref-rudin2018age" role="doc-biblioref">2018</a>)</span>, which verified the analysis conducted by ProPublica, indicating cases where the results given by COMPAS may be non-intuitive and possible explanations. The authors believe that ProPublica has drawn conclusions from incorrect assumptions and lack of knowledge of all data. For instance, they assumed linearity in age (as stated in official COMPAS documentation from Northpointe described by Rudin et al.), which appeared to be untrue; they also did not have access to the answers from questionnaires given to defendants (also introduced by the Northpointe as predictors for their model), which can be highly correlated with race and, thus, shift the outcome.</p>
<p>Disproving ProPublica’s study was not, however, the main objective of Rudin et al. They described what they believed was the real problem of COMPAS: its proprietary nature, which, along with over a hundred of (manually-entered) variables collected from questionnaires not publicly accessible, does not allow to identify data entry errors, data integration errors, missing data and other types of inaccuracies. The researchers even identified some individuals with a rich criminal history and low probability of recidivism given by model, highly suggesting that their scores were based on flawed data.
The main conclusion of this analysis was to replace black-box machine learning models by interpretable models, which, as Rudin et al. suggested, can be equally accurate for predicting recidivism.</p>
</div>
</div>
<div id="data-3" class="section level2" number="10.2">
<h2><span class="header-section-number">10.2</span> Data</h2>
<p>Data we used was mainly derived from the “The age of unfairness” study dataset combined with several factors extracted from the raw Broward County FL database used by ProPublica for their analysis.</p>
<p>We gathered information about <span class="math inline">\(5727\)</span> subjects from Broward County FL, whose likelihood of recidivism and violent recidivism we were trying to predict. We have used <span class="math inline">\(29\)</span> variables in total:</p>
<ul>
<li><strong>personal</strong> (<span class="math inline">\(6\)</span>) such as current age, age at first offence, sex, race, marital status, custody status</li>
<li><strong>criminal involvement</strong> (<span class="math inline">\(20\)</span>) consisting of number and types of previous charges and arrests, as well as those leading to COMPAS screening</li>
<li><strong>history of non-compliance</strong> (<span class="math inline">\(3\)</span>) concerning behaviour while on probation.</li>
</ul>
<p>In addition, the data also contained COMPAS scores for both recidivism and violent recidivism, together with ground truth about future offenses and violent offenses made by a person.</p>
<table>
<caption>Description of variables in the dataset.</caption>
<thead>
<tr class="header">
<th>Variable</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>current_age</code></td>
<td>current age of offender</td>
</tr>
<tr class="even">
<td><code>age_first_offence</code></td>
<td>age of offender at first offense</td>
</tr>
<tr class="odd">
<td><code>charge_count</code></td>
<td>number of charges against offender</td>
</tr>
<tr class="even">
<td><code>jail30_count</code></td>
<td>number of times in jail at least 30 days long</td>
</tr>
<tr class="odd">
<td><code>prison_sent_count</code></td>
<td>number of prison sentences</td>
</tr>
<tr class="even">
<td><code>prob_count</code></td>
<td>number of times on probation</td>
</tr>
<tr class="odd">
<td><code>race</code></td>
<td>race of offender</td>
</tr>
<tr class="even">
<td><code>gender</code></td>
<td>gender of offender</td>
</tr>
<tr class="odd">
<td><code>offense30_count</code></td>
<td>number of offenses within 30 days before screening</td>
</tr>
<tr class="even">
<td><code>fel_count</code></td>
<td>number of felonies</td>
</tr>
<tr class="odd">
<td><code>misdem_count</code></td>
<td>number of misdemeanours</td>
</tr>
<tr class="even">
<td><code>charge_viol_count</code></td>
<td>number of violent charges against offender</td>
</tr>
<tr class="odd">
<td><code>juvfel_count</code></td>
<td>number of juvenile felonies</td>
</tr>
<tr class="even">
<td><code>prop_viol_count</code></td>
<td>number of felony property violent arrests</td>
</tr>
<tr class="odd">
<td><code>murder_arrest</code></td>
<td>number of murder arrests</td>
</tr>
<tr class="even">
<td><code>felassault_arrest</code></td>
<td>number of felony assault arrests</td>
</tr>
<tr class="odd">
<td><code>misdem_arrest</code></td>
<td>number of misdemeanours assault arrests</td>
</tr>
<tr class="even">
<td><code>famviol_arrest</code></td>
<td>number of family violence arrests</td>
</tr>
<tr class="odd">
<td><code>sex_arrest</code></td>
<td>number of sex crime arrests</td>
</tr>
<tr class="even">
<td><code>weapon_arrest</code></td>
<td>number of weapons arrests</td>
</tr>
<tr class="odd">
<td><code>onprob_count</code></td>
<td>number of offenses on probation</td>
</tr>
<tr class="even">
<td><code>onprob_current</code></td>
<td>current offence on probation</td>
</tr>
<tr class="odd">
<td><code>prob_revoke</code></td>
<td>number of probation violation</td>
</tr>
<tr class="even">
<td><code>arrest_count</code></td>
<td>total number of arrests</td>
</tr>
<tr class="odd">
<td><code>prison30_count</code></td>
<td>number of times in prison at least 30 days long</td>
</tr>
<tr class="even">
<td><code>scale_set</code></td>
<td>scale set for COMPAS screening</td>
</tr>
<tr class="odd">
<td><code>marital_status</code></td>
<td>marital status</td>
</tr>
<tr class="even">
<td><code>custody_status</code></td>
<td>custody status, e.g. pretrail defendant</td>
</tr>
</tbody>
</table>
</div>
<div id="models-2" class="section level2" number="10.3">
<h2><span class="header-section-number">10.3</span> Models</h2>
<p>We focused on both direct and indirect modelling of recidivism. As in original work by Northpointe, we distinguish between recidivism and violent recidivism, giving rise to a total of <span class="math inline">\(4\)</span> problems to tackle.</p>
<p>Our first approach focused on direct modelling of recidivism and violent recidivism in a two-year follow-up period after screening. Therefore, we modelled both problems as binary classification tasks.</p>
<p>In the second approach we focused on the prediction of raw COMPAS scores, again concerning both recidivism and violent recidivism. We model it as a regression task with mean squared error as a loss function.</p>
<p>In all experiments we split our data set randomly into train set with <span class="math inline">\(4588\)</span> instances and separate test set with <span class="math inline">\(1139\)</span>. All results presented below are calculated on the test set only in order to provide fair performance analysis.</p>
<p>In order to find well-performing models for each task, we have tried three different approaches:</p>
<ul>
<li>Lasso Regression</li>
<li>Random Forest</li>
<li>Extreme Gradient Boosting (XGBoost)</li>
</ul>
<p>We focused on tuning the XGBoost model, as it provides state-of-the-art results on many tabular datasets, and wanted to compare it with a white box logistic regression model along with out-of-the-box Random Forest.</p>
<p>Comparison with a linear model can give us insights on whether the relationships between recidivism and given predictors is highly nonlinear or not. On the other hand, comparison with Random Forest model will reveal how important careful parameter tuning and control for overfitting are.</p>
<p>In next subsections, we briefly describe the fitting process of all models.</p>
<div id="xgboost" class="section level4" number="10.3.0.1">
<h4><span class="header-section-number">10.3.0.1</span> XGBoost</h4>
<p>Since the volume of our datasets is not substantial, we performed XGBoost tuning with exhaustive grid search method in a coarse-to-fine manner. With a coarse parameter search we assessed the relative importance of each parameter and narrowed down its potential range of values. Next, with a much finer search we obtained out final sets of parameters. As a metric during parameter tuning we used AUC for recidivism classification and MSE for COMPAS score prediction. Both metrics were calculated on the full training set with out-of-fold scores within a 5-fold CV scheme. In our analysis, we used <code>xgboost</code> package in R.</p>
<p>Below we show top performing models for both COMPAS score regression and recidivism binary classification.</p>
<table style="width:100%;">
<caption>Selected final parameter values for recidivism and violent recidivism XGBoost models</caption>
<colgroup>
<col width="12%" />
<col width="13%" />
<col width="12%" />
<col width="7%" />
<col width="11%" />
<col width="11%" />
<col width="18%" />
<col width="7%" />
<col width="7%" />
</colgroup>
<thead>
<tr class="header">
<th align="center">Model</th>
<th align="center">Tree Number</th>
<th align="center">Max Depth</th>
<th align="center">Eta</th>
<th align="center">Colsample</th>
<th align="center">Subsample</th>
<th align="center">Min Child Weight</th>
<th align="center">Alpha</th>
<th align="center">Lambda</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">XGB</td>
<td align="center">12</td>
<td align="center">3</td>
<td align="center">0.3</td>
<td align="center">1</td>
<td align="center">0.8</td>
<td align="center">3</td>
<td align="center">1</td>
<td align="center">1</td>
</tr>
<tr class="even">
<td align="center">XGB Violent</td>
<td align="center">19</td>
<td align="center">3</td>
<td align="center">0.3</td>
<td align="center">0.8</td>
<td align="center">0.8</td>
<td align="center">10</td>
<td align="center">1</td>
<td align="center">0</td>
</tr>
</tbody>
</table>
<p>We can see that in case of recidivism and violent recidivism classification XGBoost was prone to the overfitting as the best models had very limited tree counts.</p>
<table style="width:100%;">
<caption>Selected final parameter sets for COMPAS score and violent COMPAS score XGBoost models</caption>
<colgroup>
<col width="12%" />
<col width="13%" />
<col width="12%" />
<col width="7%" />
<col width="11%" />
<col width="11%" />
<col width="18%" />
<col width="7%" />
<col width="7%" />
</colgroup>
<thead>
<tr class="header">
<th align="center">Model</th>
<th align="center">Tree Number</th>
<th align="center">Max Depth</th>
<th align="center">Eta</th>
<th align="center">Colsample</th>
<th align="center">Subsample</th>
<th align="center">Min Child Weight</th>
<th align="center">Alpha</th>
<th align="center">Lambda</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">XGB</td>
<td align="center">62</td>
<td align="center">3</td>
<td align="center">0.3</td>
<td align="center">1</td>
<td align="center">0.8</td>
<td align="center">10</td>
<td align="center">3</td>
<td align="center">0</td>
</tr>
<tr class="even">
<td align="center">XGB Violent</td>
<td align="center">57</td>
<td align="center">3</td>
<td align="center">0.3</td>
<td align="center">0.8</td>
<td align="center">0.8</td>
<td align="center">1</td>
<td align="center">3</td>
<td align="center">0</td>
</tr>
</tbody>
</table>
<p>In case of COMPAS score regression we see a substantial increase in tree count, but still the chosen models have limited max tree depth to control for overfitting.</p>
</div>
<div id="lasso-regression" class="section level4" number="10.3.0.2">
<h4><span class="header-section-number">10.3.0.2</span> LASSO regression</h4>
<p>Since we had strong suspicions about collinearity between predictors, we used Regression with L1 penalty (Lasso) as our linear model of choice. In case of recidivism classification we fit a Logistic Lasso model - a generalized linear model with binomial distribution. In case of COMPAS regression we utilize a standard Lasso with linear activation function (gaussian distribution).</p>
<p>The penalty term <span class="math inline">\(\lambda\)</span>, similarily to XGBoost, was tuned with 5-fold CV on the training set. We utilized a well known <code>glmnet</code> package in R to efficiently find optimal penalty terms using the <em>Lasso path</em>.</p>
<p>Below we present the Lasso path for recidivism model (on the left) and COMPAS scores (on the right). We can notice that the selected model for recidivism prediction is simpler and that even simple linear models with more predictors lead to overfitting. On the other hand, we observe little to no overfitting for COMPAS regression. This is in line with parameter tuning for XGBoost models, where models selected for COMPAS score regression were more complex.</p>
<p><img src="images/01-lasso_recid.png" width="360" /> <img src="images/01-lasso_recid_compas.png" width="360" /></p>
<p><em>Left: Lasso path for recidivism model. At the top we see the number of non-zero weights. We can see the optimal penalty term chosen to be close to <span class="math inline">\(e^{-5}\)</span>. Best model had 17 non-zero weights, although models with less than 10 chosen predictors also perform well. Right: Lasso path for COMPAS regression. Optimal model selected with 26 predictors, but simpler models with less than 18 features perform similarily</em></p>
</div>
<div id="random-forest" class="section level4" number="10.3.0.3">
<h4><span class="header-section-number">10.3.0.3</span> Random Forest</h4>
<p>As for the Random Forest model, we relied on the default implementation in <code>RandomForest</code> R package, without any fine tuning. We set the number of trees to <span class="math inline">\(50\)</span> observing that the chosen XGBoost models were also sparse in tree count.</p>
</div>
<div id="results---compas-regression" class="section level3" number="10.3.1">
<h3><span class="header-section-number">10.3.1</span> Results - COMPAS Regression</h3>
<p>Below we present COMPAS regression results. In both cases of recidivism and violent recidivism tuned XGBoost models achieved highest Coefficient of determination (R-Squared). Random Forest models performed only a little worse, followed by Lasso models.</p>
<p>One may have expected higher results than <span class="math inline">\(0.643\)</span> and <span class="math inline">\(0.711\)</span> of the variance explained, for recidivism and violent recidivism respectively. Information available about the underlying COMPAS models suggest that they are most likely quite simple, statistical models. However, as disclosed by Nortpointe, their model has a substantially more information about the defender, e.g. from on-site questionnaires.</p>
<table>
<caption>Summary of COMPAS regression results</caption>
<thead>
<tr class="header">
<th align="center">Model</th>
<th align="center">R-Squared</th>
<th align="center">MSE</th>
<th align="center">RMSE</th>
<th align="center">MAD</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><strong>XGB</strong></td>
<td align="center"><strong>0.643</strong></td>
<td align="center">0.259</td>
<td align="center">0.509</td>
<td align="center">0.329</td>
</tr>
<tr class="even">
<td align="center">Lasso</td>
<td align="center">0.573</td>
<td align="center">0.310</td>
<td align="center">0.557</td>
<td align="center">0.384</td>
</tr>
<tr class="odd">
<td align="center">RF</td>
<td align="center">0.625</td>
<td align="center">0.272</td>
<td align="center">0.522</td>
<td align="center">0.336</td>
</tr>
<tr class="even">
<td align="center"><strong>XGB Violent</strong></td>
<td align="center"><strong>0.711</strong></td>
<td align="center">0.228</td>
<td align="center">0.478</td>
<td align="center">0.275</td>
</tr>
<tr class="odd">
<td align="center">Lasso Violent</td>
<td align="center">0.644</td>
<td align="center">0.281</td>
<td align="center">0.531</td>
<td align="center">0.336</td>
</tr>
<tr class="even">
<td align="center">RF Violent</td>
<td align="center">0.693</td>
<td align="center">0.242</td>
<td align="center">0.492</td>
<td align="center">0.296</td>
</tr>
</tbody>
</table>
<p>Looking more closely at the histograms of residuals, we observe a much higher amount of instances with under-valued than over-valued predictions. This observation is consistent for both recidivism and violent recidivism COMPAS scores, and for all our models.</p>
<center>
<p><img src="images/compas_residuals.png" width="370" /> <img src="images/compas_residuals_viol.png" width="370" /></p>
<p><em>Left: Histogram of residuals for COMPAS recidivism regression. Right: Histogram of residuals for violent COMPAS recidivism regression.</em></p>
</center>
</div>
<div id="results---recidivism-classification" class="section level3" number="10.3.2">
<h3><span class="header-section-number">10.3.2</span> Results - Recidivism Classification</h3>
<p>Below we present recidivism classification results. We decidecied to compare our models using a receiver operating characteristic or ROC curve in short, along with a couple other diagnostic metrics.</p>
<center>
<table>
<caption>Summary of recidivism classification results</caption>
<thead>
<tr class="header">
<th align="center">Model</th>
<th align="center">AUC</th>
<th align="center">Accuracy</th>
<th align="center">Precision</th>
<th align="center">Recall</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><strong>XGB</strong></td>
<td align="center"><strong>0.721</strong></td>
<td align="center">0.66</td>
<td align="center">0.51</td>
<td align="center">0.71</td>
</tr>
<tr class="even">
<td align="center">Lasso</td>
<td align="center">0.707</td>
<td align="center">0.65</td>
<td align="center">0.50</td>
<td align="center">0.68</td>
</tr>
<tr class="odd">
<td align="center">RF</td>
<td align="center">0.671</td>
<td align="center">0.63</td>
<td align="center">0.48</td>
<td align="center">0.64</td>
</tr>
<tr class="even">
<td align="center"><strong>XGB Violent</strong></td>
<td align="center"><strong>0.718</strong></td>
<td align="center">0.63</td>
<td align="center">0.26</td>
<td align="center">0.68</td>
</tr>
<tr class="odd">
<td align="center">Lasso Violent</td>
<td align="center">0.697</td>
<td align="center">0.65</td>
<td align="center">0.27</td>
<td align="center">0.66</td>
</tr>
<tr class="even">
<td align="center">RF Violent</td>
<td align="center">0.674</td>
<td align="center">0.63</td>
<td align="center">0.24</td>
<td align="center">0.63</td>
</tr>
</tbody>
</table>
</center>
<p>As it turned out, the most accurate algorithm was XGBoost with AUC of <span class="math inline">\(0.721\)</span> for both recidivism and violent recidivism prediction.</p>
<center>
<p><img src="images/01-roc_recid.png" width="370" /> <img src="images/01-roc_recid_violent.png" width="370" />
<em>Left: ROC curve for recidivism. Right: ROC curve for violent recidivism.</em></p>
</center>
<p>In both cases, ROC AUC was much higher than ProPublica score and also significantly higher than the ROC AUC produced by raw COMPAS scores. Surprisingly, our models trained on the COMPAS scores (COMPAS model in the table below) achieved higher AUC on the test set than the raw COMPAS scores. This may suggest, that the additional variables extracted from e.g. questionnaires, may in fact harm the COMPAS predictions.</p>
<table>
<caption>Comparison of predictive performance of our XGBoost classification models and raw COMPAS scores</caption>
<thead>
<tr class="header">
<th align="center"></th>
<th align="center">Our model</th>
<th align="center">raw COMPAS score</th>
<th align="center">COMPAS model</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">ROC AUC</td>
<td align="center"><strong>0.721</strong></td>
<td align="center">0.692</td>
<td align="center">0.708</td>
</tr>
<tr class="even">
<td align="center">ROC AUC Violent</td>
<td align="center"><strong>0.718</strong></td>
<td align="center">0.673</td>
<td align="center">0.688</td>
</tr>
</tbody>
</table>
<p>In the following analyses we will focus only on the models predicting recidivism.</p>
</div>
</div>
<div id="explanations-5" class="section level2" number="10.4">
<h2><span class="header-section-number">10.4</span> Explanations</h2>
<p>In order to evaluate results, as well as test for potential biases, we examined both COMPAS scores and our model for recidivism prediction using several explanation techniques.</p>
<p>Before we begin with explanations, however, several terms should be clarified.</p>
<p><strong>Jail or prison?</strong></p>
<p>For many, especially non-English natives, jail and prison are interchangeable terms. It is, however, incorrect: jails are for people awaiting trial and those serving short (a year or less) sentences, while prisons are for long term (more than a year) convicts.</p>
<p><strong>Misdeameanor or felony?</strong></p>
<p>A felony is a more serious crime than a misdemeanor. There are more levels within these groups and each level differs in terms of consequences, but the rule of thumb is the following: felonies involve long prison sentences, large fines, or permanent loss of freedoms, while misdemeanors usually result in smaller fines and short jail time.</p>
<div id="model-specific" class="section level3" number="10.4.1">
<h3><span class="header-section-number">10.4.1</span> Model specific</h3>
<p>In order to identify the effect of features on prediction we used Permutation Variable Importance.</p>
<center>
<p><img src="images/01-xgb_feat_imp.png" width="600" /></p>
<p><em>Permutation Variable Importance – our XGBoost model for predicting recidivism</em></p>
</center>
<p>Not surprisingly, the most relevant variable for predicting recidivism turned out to be number of previous arrests. Another factor that is greatly affecting prediction is age and age at first offense, followed by the number of previous misdemeanors.</p>
<p>In the picture below we can see a little more detailed description of most important variables acquired with SHAP. Positive SHAP value indicates larger probability of recidivism, negative – the opposite.</p>
<center>
<p><img src="images/01-shap_all.png" width="600" /></p>
<p><em>SHAP summary plot ordered by variable importance – our XGBoost model for predicting recidivism</em></p>
</center>
<p>Several interesting conclusions can be drawn.</p>
<p>Quite intuitively, the higher the number of previous arrests, the more likely is the defendant to reoffend. Features standing for current age and age at first offence indicate that younger people tend to be much more likely to commit crime again, especially if their criminal history has started early.
Higher number of previous misdemeanors affects “positively” the prediction, on the contrary to the features related to felonies (number of felony assault arrests and number of felonies), suggesting that people commiting lesser crimes are more likely to do it again, unlike those with more severe ones.</p>
<p>Unfortunately, sensitive information such as race and sex is very high on the list suggesting bias in favour of women and against African-Americans.</p>
<p>A little more digging reveals another interesting information – an interaction between race and two of the three most important variables shows the opposite direction of feature’s influence on prediction for black vs non-black defendants.</p>
<center>
<p><img src="images/01-shap_int_all_aa_age1.png" width="370" /> <img src="images/01-shap_int_all_aa_age2.png" width="370" /></p>
<p><em>Left: SHAP interactions for African-American race and age of first offense variables. Right: SHAP interactions for African-American race and current age variables. Based on our XGBoost model for predicting recidivism</em></p>
</center>
<p>The figure above suggests that younger age (both at first offence and currently) generally increases the probability of recidivism among African-Americans, while lowering it for people of Caucasian origins and vice versa.</p>
<p>This insight is particularly disturbing considering the official documentation of COMPAS algorithm stating that COMPAS scores are linearly dependent on these two variables (scaled by combination of other factors).</p>
<p>It can also mean something entirely different –particular characteristics of non-black defendants in Broward County, Florida are the reason why some of the previous analyses found COMPAS biased.</p>
<p>It is impossible to prove any of the above with the information we currently possess, thus we leave these hypotheses for further investigation.</p>
<center>
<p><img src="images/01-avg_pred_race_sex.png" width="600" /></p>
<p><em>Average prediction for race variable grouped by gender based on XGBoost model for recidivism prediction</em></p>
</center>
<p>Our model predicted African-Americans to be, on average, more likely to re-offend than individuals of any other race.</p>
<p>In order to identify, whether it is not caused by other factors correlated with race, we used Partial Dependence Plots and Accumulated Local Effects for binary variables associated with races.</p>
<p><img src="images/01-pdp_aa_2_models.png" width="310" /> <img src="images/01-pdp_cauc_2_models.png" width="310" /> <img src="images/01-labels_long_2.png" width="140" /></p>
<p><em>PDP for two race variables for regular model and model fitted on data without sex</em></p>
<p><img src="images/01-ale_aa_2_models.png" width="310" /> <img src="images/01-ale_cauc_2_models.png" width="310" /> <img src="images/01-labels_long_2.png" width="140" />
<em>ALE for two race variables for regular model and model fitted on data without sex</em></p>
</div>
<div id="instance-specific" class="section level3" number="10.4.2">
<h3><span class="header-section-number">10.4.2</span> Instance specific</h3>
<p>To further verify our claims we have performed an instance level analysis using Ceteris Paribus on observations with the most accurate predictions.</p>
<center>
<p><img src="images/01-cp_pos_corr.png" width="370" /> <img src="images/01-cp_neg_corr.png" width="370" /></p>
<p><em>Left: Ceteris Paribus plots for race and sex variables for a selected true positive instance. Right: Ceteris Paribus plots for race and sex variables for a selected true negative instance. Based on our XGBoost model for predicting recidivism</em></p>
</center>
<p>Our model has been fitted on real world data full of systematic bias, making it unfair towards African-Americans, as Ceteris Paribus freezes all the other factors.</p>
<p>Another question arises whether we observe a gender bias, because both models - ours and COMPAS - seem to drastically change their predictions for some of the male subjects when controlling for other factors.</p>
<p>From the perspective of a data scientist it might seem that bias is not such an important issue: if data shows that certain individuals are more likely to be classified as reoffending, then our models should include that information.</p>
<p>We do not know, however, how existing racial bias influenced the actual data: what if white people were simply less often arrested, had fewer charges and more let go with a warning? This scenario is not very difficult to imagine.</p>
<p>Since COMPAS is widely used by judges and parole officers, we need to include their viewpoint: they need a reliable, unbiased tool to help them determine the likelihood of individuals reoffending.</p>
<p>It is therefore our social duty to attempt at creating such a tool and do our best to make sure no existing biases are further propagated.</p>
</div>
<div id="models-without-protected-attributes" class="section level3" number="10.4.3">
<h3><span class="header-section-number">10.4.3</span> Models without protected attributes</h3>
<p>One way of dealing with the sensitive information is to remove it from a dataset. In order to examine whether this practice is of any use, we analyzed three versions of models without sensitive information: without race, without sex, and without both race and sex.</p>
<p>Results in terms of mean cross-validational ROC AUC are very similar, which might suggest that we not much information is lost by dropping these sensitive variables.</p>
<table>
<thead>
<tr class="header">
<th align="center"></th>
<th align="center">ALL</th>
<th align="center">NO RACE</th>
<th align="center">NO SEX</th>
<th align="center">NO RACE NO SEX</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><strong>ROC AUC</strong></td>
<td align="center">0.7162</td>
<td align="center">0.7154</td>
<td align="center">0.715</td>
<td align="center">0.714</td>
</tr>
</tbody>
</table>
<p>How is this possible since race was so high in the importance ranking? In the following analysis we will try to identify potential spots where this information could be hidden.</p>
<div id="model-without-race" class="section level4" number="10.4.3.1">
<h4><span class="header-section-number">10.4.3.1</span> Model without race</h4>
An XGBoost model with the same hyperparameters has been fitted to the data with race variables removed.
<center>
<p><img src="images/01-shap_norace.png" width="600" /></p>
<p><em>SHAP summary plot for model predicting recidivism without race variables</em></p>
</center>
<p>The most influencial attributes remain the same, but with no race present other variables like number of prison sentences (<strong>prison_sent_count</strong>) become much more important than before (6th position vs 11th in previous model), suggesting an association between them and race.</p>
<p>Similar conclusion comes from looking at the Partial Dependence Plot (left) and Accumulated Local Effect of this feature: its impact skyrockets for model without race.</p>
<p><img src="images/01-pdp_prison_sen.png" width="310" /> <img src="images/01-ale_prison_sen.png" width="310" /> <img src="images/01-labels_long_race_red.png" width="140" /></p>
<p><em>PDP (left) and ALE (right) plots for number of prison sentences</em></p>
</div>
<div id="model-without-sex" class="section level4" number="10.4.3.2">
<h4><span class="header-section-number">10.4.3.2</span> Model without sex</h4>
<p>Another XGBoost model with the same hyperparameters has been fitted to the data without sex variables.</p>
<center>
<p><img src="images/01-shap_nosex.png" width="600" /></p>
<p><em>SHAP summary plot for model predicting recidivism without sex variables</em></p>
</center>
<p>One of the variables potentially “hiding” the impact of sex is marital status, but the effect is much harder to notice since the dataset is very imbalanced with the majority of male defendants.
<img src="images/01-pdp_marital_so.png" width="310" /> <img src="images/01-ale_marital_so.png" width="310" /> <img src="images/01-labels_long_race_red.png" width="140" /></p>
<p><em>PDP (left) and ALE (right) plots for marital status: Significant Other</em></p>
</div>
<div id="model-without-both-race-and-sex" class="section level4" number="10.4.3.3">
<h4><span class="header-section-number">10.4.3.3</span> Model without both race and sex</h4>
<center>
<p><img src="images/01-shap_nrns.png" width="600" /></p>
</center>
<p>In this model, apart from the variables described above, a feature that suddenly got important is the number of times in prison at least 30 days (<strong>prison30_count</strong>). In the PDP (left) and ALE (right) plots below we can see another strange behaviour: models without sensitive information have strikingly different predictions for defendants with high values of this feature.</p>
<p><img src="images/01-pdp_prison_30.png" width="310" /> <img src="images/01-ale_prison_30.png" width="310" /> <img src="images/01-labels_long.png" width="140" /></p>
</div>
<div id="summary-1" class="section level4" number="10.4.3.4">
<h4><span class="header-section-number">10.4.3.4</span> Summary</h4>
<p>Removing each of the sensitive variables from the training dataset altered a little the model structure – new features climbed up the feature importance ranking. More detailed analysis revealed potential spots in which sensitive information could have “hidden”", as models drastically changed behaviour after dataset modifications. Although some of them can be harmless or even valuable adjustments, model’s creator should always have in mind potential consequences of downright removal of the attributes. In the following section we will try a different approach to bias mitigation, which does not include discarding any variables.</p>
</div>
</div>
</div>
<div id="fairness" class="section level2" number="10.5">
<h2><span class="header-section-number">10.5</span> Fairness</h2>
<div id="overview" class="section level3" number="10.5.1">
<h3><span class="header-section-number">10.5.1</span> Overview</h3>
<p>The topic of fairness in machine learning has been widely studied and has progressed intensively in recent years. One of the tools that emerged was AIF360, described in <span class="citation">Bellamy et al. (<a href="#ref-bellamy2018ai" role="doc-biblioref">2018</a>)</span> – an IBM package that assembles together the most important fairness metrics, explanation, and bias mitigating techniques, that will be used in the following analysis.</p>
</div>
<div id="models-3" class="section level3" number="10.5.2">
<h3><span class="header-section-number">10.5.2</span> Models</h3>
<p>In order to mitigate bias in our model, we used an algorithm called Prejudice Remover, described in <span class="citation">Kamishima et al. (<a href="#ref-prejudice_remover" role="doc-biblioref">2012</a>)</span>.
According to the authors,
“prejudice means a statistical dependence between a sensitive variable, S, and the target variable, Y, or a non-sensitive variable, X.”</p>
<p>We analyzed the results using the following fairness metrics:</p>
<ul>
<li><strong>Average Odds Difference</strong>
<span class="math display">\[ \frac{(FPR_{unpriv}-FPR_{priv})+(TPR_{unpriv}-TPR_{priv})}{2}\]</span></li>
<li><strong>Statistical Parity Difference</strong>, which stands for
probability of favorable outcome for unprivileged instances subtracted by probability of favorable outcome for privileged instances</li>
<li><strong>Equal Opportunity Difference</strong>, also called true positive rate difference: true positive rate on unprivileged instances minus true positive rate on privileged instances)</li>
<li><strong>Theil Index</strong> which is a special case of generalized entropy index with alpha = 1</li>
<li><strong>1-min(DI, 1/DI) </strong>
where DI stands for Disparate Impact – probability of favorable outcome for unprivileged instances divided by probability of favorable outcome for privileged instances.</li>
</ul>
<p>All metrics definitions are from AIF360 documentation.
For all of them, the value close to zero indicates a fair model.</p>
<p>Model performance was measured with balanced accuracy defined as the average of recall (ratio of true positives / (true positives + false negatives) obtained on each class. Most of the metrics were significantly lowered by the Prejudice Remover.</p>
<table>
<thead>
<tr class="header">
<th align="center">Model</th>
<th align="center">best balanced acc</th>
<th align="center">AOD</th>
<th align="center">SPD</th>
<th align="center">EOD</th>
<th align="center">TI</th>
<th align="center">1-min(DI, 1/DI)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">before PR</td>
<td align="center">0.696725</td>
<td align="center">0.245144</td>
<td align="center">0.273759</td>
<td align="center">0.276377</td>
<td align="center">0.159040</td>
<td align="center">0.4581</td>
</tr>
<tr class="even">
<td align="center">after PR</td>
<td align="center">0.630832</td>
<td align="center">0.022063</td>
<td align="center">0.051394</td>
<td align="center">0.013318</td>
<td align="center">0.223687</td>
<td align="center">0.1240</td>
</tr>
</tbody>
</table>
<p><strong>What does it mean for the defendants?</strong></p>
<p>The following analysis of mismatch was conducted using thresholds with the best balanced accuracy for both of the models on the test set with 1146 randomly chosen observations.</p>
<p>PR – model after using Prejudice Remover,
XGB – regular XGBoost model</p>
<table>
<thead>
<tr class="header">
<th align="center"></th>
<th align="center">ALL PR</th>
<th align="center">ALL XGB</th>
<th align="center">BLACK PR</th>
<th align="center">BLACK XGB</th>
<th align="center">WHITE PR</th>
<th align="center">WHITE XGB</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">Correctly classified as reoffending</td>
<td align="center">30</td>
<td align="center">293</td>
<td align="center">9</td>
<td align="center">198</td>
<td align="center">19</td>
<td align="center">73</td>
</tr>
<tr class="even">
<td align="center">Correctly classified as innocent</td>
<td align="center">720</td>
<td align="center">499</td>
<td align="center">343</td>
<td align="center">195</td>
<td align="center">281</td>
<td align="center">226</td>
</tr>
<tr class="odd">
<td align="center">Incorrectly classified as reoffending</td>
<td align="center"><strong>17</strong></td>
<td align="center"><strong>238</strong></td>
<td align="center"><strong>3</strong></td>
<td align="center"><strong>151</strong></td>
<td align="center"><strong>10</strong></td>
<td align="center"><strong>65</strong></td>
</tr>
<tr class="even">
<td align="center">Incorrectly classified as innocent</td>
<td align="center"><strong>379</strong></td>
<td align="center"><strong>116</strong></td>
<td align="center"><strong>229</strong></td>
<td align="center"><strong>40</strong></td>
<td align="center"><strong>97</strong></td>
<td align="center"><strong>43</strong></td>
</tr>
</tbody>
</table>
<p>As a result of prejudice removal we observe a large shift in classification: number of black people incorrectly classified as reoffending dropped from 151 to 3 (drop from 65 to 10 for white people), suggesting that the algorithm did in fact remove the negative bias towards them. Unfortunately, it is largely caused by PR’s aversion to classify anyone as risky of recidivism, resulting in an increase of those misclassified as innocent from 116 (for regular model) to 379.</p>
<p>Is saving potentially innocent people from being wrongfully misjudged more important than defending their community from possible future reoffences of those mistakenly classified as unlikely to reoffend?
An answer to this question should be based on a careful evaluation of potential risks of propagating biases and prejudice and gains coming from model accurately predicting likelihood of recidivism, contributing to the safer society, it is, therefore, much beyond the scope of this analysis.</p>
</div>
</div>
<div id="summary-and-conclusions-5" class="section level2" number="10.6">
<h2><span class="header-section-number">10.6</span> Summary and conclusions</h2>
<p>In this paper we focused on COMPAS scores modelling and developing our own solutions for predicting recidivism. We explored model explanations in search of potential racial and gender biases present in our model along with potential ways to guarantee model fairness.</p>
<p>Based on results, we conclude that the publicly available data is not rich enough to properly reconstruct COMPAS scores. However, our experiments show that our recidivism classification models yield better results not only than our models trained on COMPAS, but also the raw COMPAS scores, according to ROC AUC calculated on test set.</p>
<p>We also showed that the best-performing XGBoost models do indeed show racial bias, but using a Prejudice Remover approach we were able to significantly increase model fairness at the cost of reduced accuracy.</p>
<div id="next-steps" class="section level3" number="10.6.1">
<h3><span class="header-section-number">10.6.1</span> Next steps</h3>
<p>Potential next steps involve further research focused on identifying and mitigating biases – AIF360 offers several debiasing algorithms that can be applied to this problem. It would also be very beneficial to acquire new, preferably larger datasets and use them to perform all of the analyses conducted previously on data from Broward County.</p>

</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-bellamy2018ai">
<p>Bellamy, Rachel K. E., Kuntal Dey, Michael Hind, Samuel C. Hoffman, Stephanie Houde, Kalapriya Kannan, Pranay Lohia, et al. 2018. “AI Fairness 360: An Extensible Toolkit for Detecting, Understanding, and Mitigating Unwanted Algorithmic Bias.” <a href="http://arxiv.org/abs/1810.01943">http://arxiv.org/abs/1810.01943</a>.</p>
</div>
<div id="ref-prejudice_remover">
<p>Kamishima, Toshihiro, Shotaro Akaho, Hideki Asoh, and Jun Sakuma. 2012. “Fairness-Aware Classifier with Prejudice Remover Regularizer.” In <em>Machine Learning and Knowledge Discovery in Databases</em>, edited by Peter A. Flach, Tijl De Bie, and Nello Cristianini, 35–50. Berlin, Heidelberg: Springer Berlin Heidelberg.</p>
</div>
<div id="ref-propublica">
<p>Larson, Jeff, Surya Mattu, Lauren Kirchner, and Julia Angwin. 2016. “How We Analyzed the Compas Recidivism Algorithm.” Edited by ProPublica.org. <a href="hhttps://www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm">hhttps://www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm</a>.</p>
</div>
<div id="ref-rudin2018age">
<p>Rudin, Cynthia, Caroline Wang, and Beau Coker. 2018. “The Age of Secrecy and Unfairness in Recidivism Prediction.” <a href="http://arxiv.org/abs/1811.00731">http://arxiv.org/abs/1811.00731</a>.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="story-heloc-credits.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="acknowledgements.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook/js/app.min.js"></script>
<script src="libs/gitbook/js/lunr.js"></script>
<script src="libs/gitbook/js/clipboard.min.js"></script>
<script src="libs/gitbook/js/plugin-search.js"></script>
<script src="libs/gitbook/js/plugin-sharing.js"></script>
<script src="libs/gitbook/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook/js/plugin-bookdown.js"></script>
<script src="libs/gitbook/js/jquery.highlight.js"></script>
<script src="libs/gitbook/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/compstat-lmu/iml_methods_limitations/edit/master/10-story-compas.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["book.pdf", "book.epub"],
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
